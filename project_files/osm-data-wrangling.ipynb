{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "Choose any area of the world on https://www.openstreetmap.org and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data for a part of the world that you care about. Choose to learn SQL or MongoDB and apply your chosen schema to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raleigh, North Carolina, USA:\n",
    "\n",
    "- https://www.openstreetmap.org/relation/179052\n",
    "- https://mapzen.com/data/metro-extracts/metro/raleigh_north-carolina/\n",
    "\n",
    "A 9 MB unprocessed map sample can be downloaded [here](../data/raleigh_north-carolina_sample.zip).\n",
    "\n",
    "I picked this region since I'd spent almost three months there on my last business trip to the USA, more specifically to the Research Triangle. I visited quite a lot of places, but I realize there's much more I didn't get to see, so wrangling and then querying this data is a perfect opportunity to learn more about the city, if only virtually this time around. Might be I'll even have a chance to contribute to the OpenStreetMap community, who knows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the Python code I used to investigate the dataset before cleaning and uploading it to an sqlite3 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that iteratively parses the map file and counts the number of top-level tags\n",
    "def count_tags(filename):\n",
    "        tags = {}\n",
    "        for event, elem in ET.iterparse(filename):\n",
    "            t = elem.tag\n",
    "            if t in tags:\n",
    "                tags[t] += 1\n",
    "            else:\n",
    "                tags[t] = 1\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 8535,\n",
      " 'nd': 2600325,\n",
      " 'node': 2307063,\n",
      " 'osm': 1,\n",
      " 'relation': 874,\n",
      " 'tag': 872208,\n",
      " 'way': 235289}\n"
     ]
    }
   ],
   "source": [
    "# let's get a bird's eye view of the data we're dealing with\n",
    "tags = count_tags('raleigh_north-carolina.osm')\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# patterns that will help us match various <tag> 'k' values\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "problematic_tags = []\n",
    "other_tags = []\n",
    "\n",
    "# function that assigns a specific <tag> 'k' value to one of the four categories and keeps two lists of problematic and\n",
    "# unconventional 'k' values\n",
    "def key_type(element, keys):\n",
    "    global problematic_tags, other_tags\n",
    "    if element.tag == 'tag':\n",
    "        k = element.attrib['k']\n",
    "        if lower.search(k):\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.search(k):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(k):\n",
    "            problematic_tags.append(k)\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            other_tags.append(k)\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "# function that iteratively applies the key_type function to the whole map file\n",
    "def process_map(filename):\n",
    "    keys = {'lower': 0, 'lower_colon': 0, 'problemchars': 0, 'other': 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 543051, 'lower_colon': 285435, 'other': 43720, 'problemchars': 2}\n"
     ]
    }
   ],
   "source": [
    "# let's see how the 'k' values are distributed among the four groups\n",
    "keys = process_map('raleigh_north-carolina.osm')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most values seem to be alright, but two of them contain some problematic characters - it's worth investigating what those are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service area', 'chair lift']\n"
     ]
    }
   ],
   "source": [
    "print problematic_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like those two both have a whitespace that invalidates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnis:Class\n",
      "Street_1\n",
      "tiger:zip_right_1\n",
      "name_1\n",
      "tiger:name_type_1\n",
      "nhd-shp:fcode\n",
      "nhd-shp:fdate\n",
      "nhd-shp:fcode\n",
      "nhd-shp:fdate\n",
      "nhd-shp:fcode\n",
      "nhd-shp:fdate\n",
      "nhd-shp:fcode\n",
      "nhd-shp:fcode\n",
      "nhd-shp:fdate\n",
      "nhd-shp:com_id\n",
      "NHD:FCode\n",
      "NHD:RESOLUTION\n",
      "NHD:FCode\n",
      "NHD:ComID\n",
      "NHD:FTYPE\n",
      "NHD:ComID\n",
      "NHD:FCode\n",
      "NHD:RESOLUTION\n",
      "NHD:ReachCode\n",
      "NHD:RESOLUTION\n",
      "NHD:way_id\n",
      "NHD:ComID\n",
      "NHD:FCode\n",
      "NHD:way_id\n",
      "NHD:FCode\n",
      "NHD:FCode\n",
      "NHD:FCode\n",
      "NHD:RESOLUTION\n",
      "NHD:ComID\n",
      "NHD:FDate\n",
      "NHD:ReachCode\n",
      "NHD:FDate\n",
      "NHD:GNIS_ID\n",
      "tiger:name_base_1\n",
      "tiger:zip_right_1\n",
      "nhd-shp:fdate\n",
      "nhd-shp:fcode\n",
      "tiger:name_base_1\n",
      "NHD:ReachCode\n"
     ]
    }
   ],
   "source": [
    "# what about other_tags 'k' values?\n",
    "# we'll be looking at every 1000th item since there're over 40K of them\n",
    "for i, o in enumerate(other_tags):\n",
    "    if i % 1000 == 0: print o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data seemingly comes from at least three different sources: Tiger GPS, National Hydrography Dataset (NHD), and Geographic Names Information System (GNIS) - judging by the prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = 'raleigh_north-carolina.osm'\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE) # grabs the street type from a given string\n",
    "expected = ['Street', 'Avenue', 'Boulevard', 'Drive', 'Court', 'Place',\n",
    "            'Square', 'Lane', 'Road', 'Trail', 'Parkway', 'Commons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that checks if a given street type conforms to one of the expected values\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "# function that checks if an element contains a street name\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == 'addr:street')\n",
    "\n",
    "# function that checks all the street types in a given OSM file and keeps track of all those not in line with the expected values\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, 'r')\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start',)):\n",
    "        if elem.tag == 'node' or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100': set(['100', 'Westgate Park Dr #100']),\n",
      " '1000': set(['Six Forks Road #1000']),\n",
      " '102': set(['NE Maynard Rd #102', 'Page Rd #102']),\n",
      " '17': set(['US Highway 17']),\n",
      " '206': set(['Barrett Dr Suite 206']),\n",
      " '27609': set(['27609']),\n",
      " '27609-5360': set(['27609-5360']),\n",
      " '315': set(['Kildaire Farm Road #315']),\n",
      " '501': set(['US 15;US 501']),\n",
      " '54': set(['Highway 54',\n",
      "            'State Highway 54',\n",
      "            'West Highway 54',\n",
      "            'West NC Highway 54',\n",
      "            'West State Highway 54']),\n",
      " '55': set(['Highway 55', 'NC Highway 55', 'US 55']),\n",
      " '70': set(['US 70']),\n",
      " '751': set(['NC 751', 'NC Highway 751']),\n",
      " 'Alley': set(['Yates Motor Company Alley']),\n",
      " 'Ave': set(['Atlantic Ave',\n",
      "             'E. Winmore Ave',\n",
      "             'East Winmore Ave',\n",
      "             'Glenwood Ave',\n",
      "             'Mountford Ave',\n",
      "             'S Boylan Ave',\n",
      "             'S. Boylan Ave']),\n",
      " 'Blvd': set(['Airport Blvd',\n",
      "              'Capital Blvd',\n",
      "              'Fordham Blvd',\n",
      "              'Lake Grove Blvd',\n",
      "              'Martin Luther King Jr Blvd',\n",
      "              'Martin Luther King Jr. Blvd',\n",
      "              'Martin Luther King Junior Blvd',\n",
      "              'Southpoint Autopark Blvd']),\n",
      " 'Blvd.': set(['Durham-Chapel Hill Blvd.',\n",
      "               'N Fordham Blvd.',\n",
      "               'Southpoint Auto Park Blvd.']),\n",
      " 'Bypass': set(['Highway 54 Bypass', 'US 15 501 Bypass']),\n",
      " 'CIrcle': set(['Meadowmont Village CIrcle']),\n",
      " 'Chatham': set(['East Chatham']),\n",
      " 'Cir': set(['Braddock Cir']),\n",
      " 'Circle': set(['Alliance Circle',\n",
      "                'Arrow Leaf Circle',\n",
      "                'Barriedale Circle',\n",
      "                'Cottonwood Circle',\n",
      "                'Croatan Circle',\n",
      "                'Davie Circle',\n",
      "                'Davis Circle',\n",
      "                'Davis Grove Circle',\n",
      "                'Duck Pond Circle',\n",
      "                'Euphoria Circle',\n",
      "                'Glen Alpine Circle',\n",
      "                'Golden Horseshoe Circle',\n",
      "                'Hayloft Circle',\n",
      "                'Kevin Circle',\n",
      "                'Kintyre Circle',\n",
      "                'Lake Hollow Circle',\n",
      "                'Langley Circle',\n",
      "                'Meadowmont Village Circle',\n",
      "                'Meadowvale Circle',\n",
      "                'Medallion Circle',\n",
      "                'Mount Rogers Circle',\n",
      "                'Noblewood Circle',\n",
      "                'Northwood Circle',\n",
      "                'Page Point Circle',\n",
      "                'Parkbrook Circle',\n",
      "                'Parkleaf Circle',\n",
      "                'Parkmist Circle',\n",
      "                'Parkmount Circle',\n",
      "                'Parkvine Circle',\n",
      "                'Penley Circle',\n",
      "                'Phaeton Circle',\n",
      "                'Pine Top Circle',\n",
      "                'Piper Stream Circle',\n",
      "                'Quarrystone Circle',\n",
      "                'Ravens Point Circle',\n",
      "                'Redbud Circle',\n",
      "                'Redwood Circle',\n",
      "                'Ricky Circle',\n",
      "                'Rockland Circle',\n",
      "                'Shady Meadow Circle',\n",
      "                'Sunset Creek Circle',\n",
      "                'Talton Circle',\n",
      "                'Tartan Circle',\n",
      "                'Viewpoint Circle']),\n",
      " 'Crescent': set(['West Acres Crescent']),\n",
      " 'Crossing': set(['Oldham Forest Crossing']),\n",
      " 'Ct': set(['Ashley Springs Ct', 'Bevel Ct', 'Braidwood Ct', 'Gulf Ct']),\n",
      " 'Dr': set(['Calabria Dr',\n",
      "            'Colonnade Center Dr',\n",
      "            'Crab Orchard Dr',\n",
      "            'Gold Star Dr',\n",
      "            'Kentington Dr',\n",
      "            'Sablewood Dr',\n",
      "            'Stinson Dr',\n",
      "            'Triangle Plantation Dr',\n",
      "            'University Dr',\n",
      "            'Waterford Lake Dr',\n",
      "            'Westgate Dr']),\n",
      " 'Dr.': set(['Davis Dr.']),\n",
      " 'Driver': set(['Garrett Driver']),\n",
      " 'E': set(['Garner Rd Ste E']),\n",
      " 'East': set(['US Highway 70 East']),\n",
      " 'Ext': set(['New Hope Commons Boulevard Ext', 'Pritchard Avenue Ext']),\n",
      " 'Extension': set(['Weaver Dairy Road Extension']),\n",
      " 'Fork': set(['Dry Fork']),\n",
      " 'Grove': set(['Newton Grove']),\n",
      " 'Highway': set(['Apex Highway', 'NC 54 Highway', 'Wake Forest Highway']),\n",
      " 'Hill': set(['Chapel Hill']),\n",
      " 'Hills': set(['The Circle at North Hills']),\n",
      " 'LaurelcherryStreet': set(['LaurelcherryStreet']),\n",
      " 'Ln': set(['Springerly Ln', 'Sunbow Falls Ln']),\n",
      " 'Loop': set(['Amiable Loop', 'Farrow Glen Loop']),\n",
      " 'PI': set(['Alexander Promenade PI']),\n",
      " 'Pkwy': set(['McCrimmon Pkwy', 'Skyland Ridge Pkwy']),\n",
      " 'Pky': set(['Brier Creek Pky', 'brier Creek Pky']),\n",
      " 'Pl': set(['Balaji Pl']),\n",
      " 'Plaza': set(['City Hall Plaza', 'Exchange Plaza', 'Park Forty Plaza']),\n",
      " 'Point': set(['Rocky Point']),\n",
      " 'Practice': set(['Triangle Family Practice']),\n",
      " 'Rd': set(['Blue Ridge Rd',\n",
      "            'Buck Jones Rd',\n",
      "            'Chapel Hill Rd',\n",
      "            'Creedmoor Rd',\n",
      "            'Hillandale Rd',\n",
      "            'N Roxboro Rd',\n",
      "            'Six Forks Rd',\n",
      "            'Trinity Rd']),\n",
      " 'Rd.': set(['Bayleaf Church Rd.', 'Hillandale Rd.']),\n",
      " 'Ridge': set(['Vilana Ridge']),\n",
      " 'Run': set(['Deep Gap Run', 'Morgans Corner Run']),\n",
      " 'ST': set(['W EDENTON ST']),\n",
      " 'St': set(['9th St',\n",
      "            'Hillsborough St',\n",
      "            'Holloway St',\n",
      "            'Kinsey St',\n",
      "            'Main at North Hills St',\n",
      "            'W Franklin St']),\n",
      " 'St,': set(['Morris St,']),\n",
      " 'St.': set(['E Rosemary St.',\n",
      "             'East Corcoran St.',\n",
      "             'East Franklin St.',\n",
      "             'W Rosemary St.',\n",
      "             'W. Franklin St.',\n",
      "             'W. Pettigrew St.',\n",
      "             'West Rosemary St.']),\n",
      " 'Terrace': set(['Crestgate Terrace', 'Stonebrook Terrace', 'Vernon Terrace']),\n",
      " 'Way': set(['Anson Way',\n",
      "             'Attmore Way',\n",
      "             'Augustus Merrimon Way',\n",
      "             'Bergeron Way',\n",
      "             'Boulderstone Way',\n",
      "             'Bright Beginning Way',\n",
      "             'Bromfield Way',\n",
      "             'Brook Fern Way',\n",
      "             'Bullybunion Way',\n",
      "             'Burgwin Wright Way',\n",
      "             'Calebra Way',\n",
      "             'Centre Green Way',\n",
      "             'Chessridge Way',\n",
      "             'Cupola Chase Way',\n",
      "             'Drystack Way',\n",
      "             'Duncan Vale Way',\n",
      "             'Endeavor Way',\n",
      "             'Environ Way',\n",
      "             'Glebe Way',\n",
      "             'Graywick Way',\n",
      "             'Heck Andrews Way',\n",
      "             'Hill Hollow Way',\n",
      "             'Holsten Bank Way',\n",
      "             'Initiative Way',\n",
      "             'Katahdin Way',\n",
      "             'Kate Denson Way',\n",
      "             'Kenwyck Manor Way',\n",
      "             'Lathbury Landing Way',\n",
      "             'Leacroft Way',\n",
      "             'Ledgestone Way',\n",
      "             'Lone Star Way',\n",
      "             'Lothian Way',\n",
      "             'Marlborough Way',\n",
      "             'Mentone Way',\n",
      "             'Michaels Way',\n",
      "             'Montclair Way',\n",
      "             'Morrisville Square Way',\n",
      "             'Mt Pisgah Way',\n",
      "             'Natalie Brook Way',\n",
      "             'Newington Hills Way',\n",
      "             'Olde Weatherstone Way',\n",
      "             'Partners Way',\n",
      "             'Passaic Way',\n",
      "             'Plank Bridge Way',\n",
      "             'Pond Glen Way',\n",
      "             'Rossburn Way',\n",
      "             'Sorrell Brook Way',\n",
      "             'Talisman Way',\n",
      "             'Thendara Way',\n",
      "             'Trent Woods Way',\n",
      "             'Union Mills Way',\n",
      "             'Winifred Way',\n",
      "             'Wyatt Brook Way',\n",
      "             'Zaldivar Way']),\n",
      " 'West': set(['Columbia Place West',\n",
      "              'Highway 54 West',\n",
      "              'Highway 55 West',\n",
      "              'Highway West',\n",
      "              'NC Highway 55 West'])}\n"
     ]
    }
   ],
   "source": [
    "# running this will display all unique street types, along with street names, that don't match anything we have in\n",
    "# the expected list\n",
    "st_types = audit(OSMFILE)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Naturally enough, given a large number of contributors to the OSM project, there's a certain variation in the spelling of some of the most common street types: street (ST, St, St.), road (Rd, Rd.), parkway (Pkwy, Pky), drive (Dr, Dr.) to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problems Identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I was playing with the map data, I encountered two major problems that I had to deal with:\n",
    "\n",
    "1. I've already mentioned this issue above: some street types have several abbreviated versions or simply contain typos, which breeds inconsistency.\n",
    "2. A lot of data in the map doesn't seem to describe Raleigh itself, but rather surrounding areas.\n",
    "\n",
    "I'm going to elaborate on both these items below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent Street Type Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the *audit* function provided above, you'll see records like 'Capital Blvd', 'Southpoint Auto Park Blvd.', 'Meadowmont Village CIrcle', or 'W Rosemary St.'. For the sake of consistency, I decided to spell out each name in full, so that, for example, 'W Rosemary St.' becomes 'West Rosemary Street'. However, I realized I needed to be extra careful with cases like 'Ste N' (for it not to be turned into 'Suite North') - hence an additional check in the if statement in the *update_name* function below.\n",
    "\n",
    "So, I created a mapping from the \"incorrect\" spelling to the correct one and then fixed it on the fly while iterating over the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'ST': 'Street', 'St': 'Street', 'St.': 'Street', 'St,': 'Street', 'Rd.': 'Road', 'Rd': 'Road', 'Pl': 'Place',\n",
    "          'PI' : 'Place', 'Pky': 'Parkway', 'Pkwy': 'Parkway', 'Ln': 'Lane', 'Dr': 'Drive', 'Dr.': 'Drive', 'Driver': 'Drive',\n",
    "          'Ct': 'Court', 'Cir': 'Circle', 'CIrcle': 'Circle', 'Blvd.': 'Boulevard', 'Blvd': 'Boulevard', 'Ave': 'Avenue',\n",
    "          'Ste': 'Suite', 'Ext': 'Extension', 'W': 'West', 'E': 'East', 'N': 'North', 'S': 'South'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function corrects the street type spelling while the data is being prepared to be put into CSV files\n",
    "def update_name(name, mapping):\n",
    "    name = name.split()\n",
    "    for n in xrange(len(name)):\n",
    "        if name[n] in mapping and name[n - 1] not in ('Ste', 'Ste.', 'Suite'):\n",
    "            name[n] = mapping[name[n]]\n",
    "    return ' '.join(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Misplaced\" Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I started working on this project, I did some reading on Raleigh and found out all the zip codes in the city start with 276. However, in the course of querying the database that contained the processed data I noticed some zip codes don't match this pattern, so I wrote the following query to see what the top 20 (purely arbitrary number) zip codes are:\n",
    "\n",
    "```SQL\n",
    "SELECT tags.value, COUNT(*) cnt \n",
    "FROM (SELECT * FROM ways_tags \n",
    "      UNION \n",
    "      SELECT * FROM nodes_tags) tags\n",
    "WHERE tags.key = 'postcode'\n",
    "GROUP BY tags.value\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 20;\n",
    "```\n",
    "\n",
    "It selects records from two tables (*ways_tags* and *nodes_tags*) and groups them by postal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this query, in descending order, are below:\n",
    "\n",
    "\n",
    "zip code|count\n",
    "--------|---\n",
    "27560|1567\n",
    "27519|899\n",
    "27701|685\n",
    "27609|674\n",
    "27705|518\n",
    "27615|388\n",
    "27510|328\n",
    "27514|179\n",
    "27513|158\n",
    "27511|113\n",
    "27606|106\n",
    "27707|99\n",
    "27516|96\n",
    "27601|94\n",
    "27517|74\n",
    "27703|68\n",
    "27704|62\n",
    "27612|59\n",
    "27713|51\n",
    "27617|45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20 zip codes, less than a third (6, to be precise) start with 276 and thus belong to the city of Raleigh, which means the map file contains large areas outside the city limits. Let's now see what those areas are with the help of the following aggregation:\n",
    "\n",
    "```SQL\n",
    "SELECT tags.value, COUNT(*) cnt \n",
    "FROM (SELECT * FROM ways_tags\n",
    "      UNION\n",
    "      SELECT * FROM nodes_tags) tags\n",
    "WHERE tags.key = 'city'\n",
    "GROUP BY tags.value\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10; /* can be dropped, as we'll later see */\n",
    "```\n",
    "\n",
    "Like the query above, it selects records from two tables (*ways_tags* and *nodes_tags*), looks at those that have 'city' in the 'key' column, and groups these records by city name (that is the value in the 'value' column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ten cities produced by this query are as follows:\n",
    "\n",
    "1. Raleigh\n",
    "2. Cary\n",
    "3. Morrisville\n",
    "4. Durham\n",
    "5. Chapel Hill\n",
    "6. Carrboro\n",
    "7. Research Triangle Park\n",
    "8. Wake Forest\n",
    "9. chapel Hill (sic!)\n",
    "10. durham (sic!)\n",
    "\n",
    "What jumps right out at you is the mistyped city names (9 and 10), which, however, can easily be fixed with a query of the following form:\n",
    "\n",
    "```SQL\n",
    "UPDATE <table_name> SET value = <correct_city_name> WHERE value = <mistyped_name>;\n",
    "```\n",
    "\n",
    "I actually had to run it quite a few times (6 or 7) before I got the error-free results, so I didn't include the exact values, but provided a template instead. The final top-1o (which is not entirely true) list looks like this:\n",
    "\n",
    "1. Raleigh\n",
    "2. Cary\n",
    "3. Morrisville\n",
    "4. Durham\n",
    "5. Chapel Hill\n",
    "6. Carrboro\n",
    "7. Research Triangle Park\n",
    "8. Wake Forest\n",
    "9. Apex\n",
    "\n",
    "There are \"only\" 9 unique cities in the dataset, after all! So, we can omit the 'LIMIT 10' line from the query we used to obtain this list.\n",
    "\n",
    "Now, it becomes more evident the map includes Raleigh-Durham-Chapel Hill Combined Statistical Area (CSA) but is not confined to it, encompassing even a few satellite towns like Cary and Morrisville.\n",
    "\n",
    "On the one hand, come to think of it, it does make sense to include this data together since all these cities are very tightly tied both economically and politically. On the other hand, it's surprising and even somewhat perplexing to find so much unexpected data in a map file clearly marked as describing one city - Raleigh.\n",
    "\n",
    "I can see two possible ways of solving this issue: either separate this map file into distinct cities (it would reduce the size of the corresponding files and make for simpler processing) - one city per file - or give the file a more descriptive name to avoid potential confusion and misunderstanding among contributors and people using these map files for their own purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section contains high-level data stats and the corresponding SQL queries used to gather them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### File Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "File name|Size\n",
    "---------|----\n",
    "Raleigh.db|249 MB\n",
    "raleigh_north-carolina.osm|456 MB\n",
    "nodes.csv|178 MB\n",
    "nodes_tags.csv|1.97 MB\n",
    "ways.csv|12.9 MB\n",
    "ways_nodes.csv|56.9 MB\n",
    "ways_tags.csv|27.9 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Exploratory Code** section, I used the *count_tags* function to count the number of unique tags and got this result: 2,307,063 node tags and 235,289 way tags, among others. Let's now use SQL to see if new numbers match what Python gave us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Number of Way Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The query is pretty straightforward:\n",
    "    \n",
    "```SQL\n",
    "SELECT COUNT(*) FROM ways;\n",
    "```\n",
    "\n",
    "The result is indeed 235,289!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Node Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is change the name of the table in the query above:\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*) FROM nodes;\n",
    "```\n",
    "\n",
    "And this query yields 2,307,063."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just select and combine unique user ids from two tables like this:\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(DISTINCT(users.uid))          \n",
    "FROM (SELECT uid FROM nodes\n",
    "      UNION\n",
    "      SELECT uid FROM ways) users;\n",
    "```\n",
    "\n",
    "Turns out 821 people have contributed to this map so far. Wouldn't it be nice to know the top contributors? Let's do just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below selects records from the *nodes* and *ways* tables and groups them by user name, effectively producing the number of contributions each of the top 10 users made:\n",
    "\n",
    "```SQL\n",
    "SELECT users.user, COUNT(*) as cnt          \n",
    "FROM (SELECT user FROM nodes\n",
    "      UNION ALL\n",
    "      SELECT user FROM ways) users\n",
    "GROUP BY users.user\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "The top-10 list is as follows:\n",
    "\n",
    "user name|contributions\n",
    "---------|-------------\n",
    "jumbanho|1571998\n",
    "JMDeMai|178885\n",
    "bdiscoe|127604\n",
    "woodpeck_fixbot|114991\n",
    "bigal945|103478\n",
    "yotann|66862\n",
    "runbananas|41623\n",
    "sandhill|32512\n",
    "MikeInRaleigh|30150\n",
    "Clay Hobbs|22107\n",
    "\n",
    "\n",
    "jumbanho is way ahead of the competition with 1.5 million contributions, which is almost 10 times more than the runner-up and definitely overshadows contributors 2-10 combined!\n",
    "\n",
    "It's also worth exploring user activity by year to see when the community members working on this map were the most active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just need to get the year from the 'timestamp' column and group the records from two tables by year:\n",
    "\n",
    "```SQL\n",
    "SELECT STRFTIME('%Y', users.timestamp), COUNT(*) as cnt          \n",
    "FROM (SELECT timestamp FROM nodes\n",
    "      UNION ALL\n",
    "      SELECT timestamp FROM ways) users\n",
    "GROUP BY STRFTIME('%Y', users.timestamp)\n",
    "ORDER BY cnt DESC;\n",
    "```\n",
    "\n",
    "Here's what the query returns:\n",
    "\n",
    "year|contributions\n",
    "----|-------------\n",
    "2010|951368\n",
    "2009|587266\n",
    "2015|301073\n",
    "2016|173996\n",
    "2013|168398\n",
    "2011|147953\n",
    "2012|112006\n",
    "2014|96906\n",
    "2008|2969\n",
    "2007|417\n",
    "\n",
    "The main takeaway from these results is that this map was created 9 years ago, gaining popularity among users year after year, with major spikes in 2009 and then 2010 (when contributors were the most active) - then the user activity slumped in 2011 and experienced a few ups and downs over the years.\n",
    "\n",
    "Below is a chart for visual support:\n",
    "\n",
    "![](../img/contributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains several SQL queries I used to gain a deeper insight into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Types of Cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my time in Raleigh, I noticed that, apart from traditional American pizza and burger places, there were lots of cafes offering Mexican and Italian cuisine. Let's now see what types of cuisine are the most popular in this area:\n",
    "\n",
    "```SQL\n",
    "SELECT tags.value, COUNT(*) cnt\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "      UNION ALL\n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key = 'cuisine'\n",
    "GROUP BY tags.value\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Here are the results in descending order of popularity:\n",
    "\n",
    "type of cuisine|number of places\n",
    "---------------|----------------\n",
    "burger|88\n",
    "sandwich|59\n",
    "mexican|53\n",
    "american|52\n",
    "pizza|52\n",
    "coffee_shop|33\n",
    "italian|32\n",
    "chicken|29\n",
    "chinese|29\n",
    "japanese|18\n",
    "\n",
    "Indeed, both Mexican and Italian food did make the top-10 list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, cafes and restaurants are obviously pretty popular places, but what can the data tell us about other types of amenities? Let's select records from the *nodes_tags* and *ways_tags* tables and group them by the type of amenity:\n",
    "\n",
    "```SQL\n",
    "SELECT tags.value, COUNT(*) cnt\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "      UNION ALL\n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key = 'amenity'\n",
    "GROUP BY tags.value\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Top 10 results look like this:\n",
    "\n",
    "amenity|number of entries\n",
    "-------|-----------------\n",
    "parking|2334\n",
    "bicycle_parking|576\n",
    "restaurant|558\n",
    "place_of_worship|537\n",
    "fast_food|285\n",
    "school|228\n",
    "fuel|215\n",
    "bench|132\n",
    "waste_basket|118\n",
    "bank|116\n",
    "\n",
    "Quite a few places of worship out there! A logical enough step further would be to check what religions are represented in the area. While the number one answer is more or less predictable, it's still interesting to see what distribution the data offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Religion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "SELECT tags.value, COUNT(*) cnt\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "      UNION ALL\n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key = 'religion'\n",
    "GROUP BY tags.value\n",
    "ORDER BY cnt DESC;\n",
    "```\n",
    "\n",
    "Here's what we get:\n",
    "\n",
    "religion|number of entries\n",
    "--------|-----------------\n",
    "christian|492\n",
    "muslim|6\n",
    "jewish|5\n",
    "unitarian_universalist|2\n",
    "bahai|1\n",
    "eckankar|1\n",
    "hindu|1\n",
    "sai_baba|1\n",
    "\n",
    "Given the number of Hindus in the USA, I'm surprised to see Hinduism so severely underrepresented in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to understand the limitations of the analysis performed in this section. The data is apparently incomplete and also includes areas outside Raleigh, which leads to certain data skewness. So, we're limited to making **tentative** conclusions about the data and shouldn't try to extrapolate what modest findings we have on hand to larger regions (like the entire city of Raleigh, the state of North Carolina, or even the whole territory of the USA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having worked with the map data for some time now, here are my key takeaways from this project, along with some improvement suggestions:\n",
    "\n",
    "1. The data contains lots of typos (take city names as an example), and some values (like street types) just have multiple spellings, which breeds inconsistency, increases the overall size of the dataset, and, on the whole, makes it harder to work with it. It would be extremely useful to have some automatic checks (based on the rules agreed on by the core OSM community; for example: can all caps be used in records?; what's more acceptable, 'Street' or 'St' or 'St.'? and so on) in place that would prevent problematic entries from making their way into the map data. Of course, it would take a large-scale discussion to establish these filtering rules and may be problematic due to contributors being scattered all over the US and possibly the world. To make it more feasible, the core community could appoint several trusted representatives to a committee whose primary goal would be to come up with the rules in question.\n",
    "2. Despite being named 'Raleigh', this map file includes a much larger area, encompassing several neighboring cities, which did take me aback the first time I looked at the data. While the reasons behind the choice to include extra areas might be clear (say, close socio-economic ties), it's still confusing to have all these cities jumbled together. I believe it would be more convenient, both from the logical and computational standpoints, to have a separate file for each city, however small it might be. One major concern here is the time and effort involved in performing this task, so the question still remains - who's going to take it upon themselves to do the splitting? Would it be a single contributor skilled enough in scripting to do it programmatically? Or are several contributors from the corresponding areas better suited for the job?\n",
    "3. To grow the community and involve new members more actively, spurring competition among the contributors sounds like a good idea to me. It could be achieved by introducing some kind of a leaderboard (could be global or by region/country/city, or both) and awarding honor badges for various achievements (like the most valuable contribution (would be chosen based on the number of likes or upvotes it receives from the community), the highest number of edits in one day/month/year and anything along these lines). On the other hand, users that systematically disregard the rules and standards set out by the community could somehow be penalized or suspended from contributing for some time. The last suggestion may seem a little harsh and could potentially put off some of the contributors, but I strongly believe it's a necessary evil if the community's primary goal is to have well-cleaned, reliable, unambiguous, and easy-to-use maps.\n",
    "4. This point should be studied more closely, but I noticed that some data has prefixes that indicate it comes from sources like Tiger GPS, National Hydrography Dataset (NHD), and Geographic Names Information System (GNIS). And this data was highly likely added to the map file automatically (why would a person add all these prefixes manually in the first place?), by a scraper or a crawler created by a skilled programmer. If the community could leverage the expertise of such people, combined with the automatic rule-based filters I proposed in item 1, this map file (and not only this, but potentially all of them) could be greatly improved given a programmatically generated influx of well-cleaned data that requires minimum interference from human editors. The only issue I see here (apart from the one I highlighted in item 1 above) is that all the automation is done by humans and is thus error-prone by default, so this initiative may require quite a few skillful programmers to meticulously double-check everything and to maintain the whole data pipeline as well as make adjustments and improvements to it as new data sources appear and as data itself is evolving."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
